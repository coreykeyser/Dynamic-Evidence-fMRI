subjects=dim(resp)[2]
n.trials=dim(resp)[1]
s.hfar=matrix(NA,subjects,2)
dprime<-bias<-numeric(subjects)
for(i in 1:subjects){
sig<-noise<-hit<-fa<-m.sig<-m.noise<-m.hit<-m.fa<-0
for(j in 1:n.trials){
if(truth[j,i]==1)sig=sig+1
if(truth[j,i]==0)noise=noise+1
if(truth[j,i]==1 && resp[j,i]==1)hit=hit+1
if(truth[j,i]==0 && resp[j,i]==1)fa=fa+1
}
s.hfar[i,1]=hit/sig
s.hfar[i,2]=fa/noise
dprime[i]=qnorm(s.hfar[i,1])-qnorm(s.hfar[i,2])
bias[i]=exp(-((s.hfar[i,1])^2-qnorm(1-s.hfar[i,2])^2)/2)
}
list(hitrate=s.hfar[,1],farate=s.hfar[,2],dprime=dprime,bias=bias)
}
# slightly vectorized REM code.
oldway1=function(u=u,c=c,g=g,w=w,n.study,n.targets,n.test){
n.distractors=n.test-n.targets
v=matrix(0,w,n.study)
# generate the study list
study=matrix(rgeom(w*n.study,g)+1,w,n.study)
# set up the test list
targets=study[,1:n.targets]
distractors=matrix(rgeom(w*n.distractors,g)+1,w,n.distractors)
probe=cbind(targets,distractors)
lambda=matrix(NA,n.study,n.test)
last=array(NA,c(w,n.study,n.test))
# this is the storage process in REM at study
for(j in 1:n.study){
for(k in 1:w){
flip=0
if(flip==0){
if(runif(1)<u){
flip=1
if(runif(1)<c){v[k,j]=study[k,j]} else {v[k,j]=rgeom(1,g)+1}
}}
}}
# this is the feature comparison process at test
for(i in 1:n.test){
for(j in 1:n.study){
for(k in 1:w){
if(v[k,j]==0)last[k,j,i]=1
if(v[k,j]!=0){
if(v[k,j]==probe[k,i]){last[k,j,i]=(c+(1-c)*g*(1-g)^(v[k,j]-1))/(g*(1-g)^(v[k,j]-1))
} else {last[k,j,i]=1-c}
}}
lambda[j,i]=prod(last[,j,i])
}}
# Bayesian decision rule
as.numeric(apply(lambda,2,mean)>1)
}
oldway2=function(u=u,c=c,g=g,w=w,n.study,n.targets,n.test){
n.distractors=n.test-n.targets
v=numeric(w*n.study)
study=numeric(w*n.study)
distractors=numeric(w*n.distractors)
probe=numeric(w*n.test)
fam=out=numeric(n.test)
lambda=numeric(n.study)
targets=numeric(n.targets)
# this is the study list
for(i in 1:w){
for(j in 1:n.study){
study[(i-1)*n.study+j]=rgeom(1,g)+1
}}
# these are the items from study that will be presented at test
for(i in 1:w){
for(j in 1:n.targets){
targets[(i-1)*n.targets+j]=study[(i-1)*n.targets+j]
}}
# these are the items that are presented at test, not from the study list
for(i in 1:w){
for(j in 1:n.distractors){
distractors[(i-1)*n.distractors+j]=rgeom(1,g)+1
}}
# the next two blocks constructs the test list. targets go first, then distractors
for(i in 1:w){
for(j in 1:n.targets){
probe[(i-1)*n.targets+j]=study[(i-1)*n.targets+j]
}}
for(i in 1:w){
for(j in 1:n.distractors){
probe[(i-1)*n.distractors+j+n.targets*w]=distractors[(i-1)*n.distractors+j]
}}
# this is the storage process in REM at study
for(i in 1:n.study){
for(j in 1:w){
r1=runif(1.0)
r2=runif(1.0)
if(r1>=u)v[(i-1)*w+j]=0.0
if(r1<u & r2<c)v[(i-1)*w+j]=study[(i-1)*w+j]
if(r1<u & r2>=c)v[(i-1)*w+j]=rgeom(1,g)+1
}}
# this is the feature comparison process at test
for(i in 1:n.test){
lambda=0
for(j in 1:n.study){
odds=1
for(k in 1:w){
if(v[(j-1)*w+k]==0)temp=1
if(v[(j-1)*w+k]!=0 & v[(j-1)*w+k]==probe[(i-1)*w+k])temp=(c+(1-c)*g*(1-g)^(v[(j-1)*w+k]-1))/(g*(1-g)^(v[(j-1)*w+k]-1))
if(v[(j-1)*w+k]!=0 & v[(j-1)*w+k]!=probe[(i-1)*w+k])temp=1-c
odds=odds*temp
}
lambda=lambda+odds
}
# calculation of familiarity
fam[i]=1/n.study*lambda
# Bayesian decision rule
if(fam[i]>=1)out[i]=1
if(fam[i]<1)out[i]=0
}
out
}
dprime=function(resp,truth){
truth=as.matrix(truth)
resp=as.matrix(resp)
subjects=dim(resp)[2]
n.trials=dim(resp)[1]
s.hfar=matrix(NA,subjects,2)
dprime<-bias<-numeric(subjects)
for(i in 1:subjects){
sig<-noise<-hit<-fa<-m.sig<-m.noise<-m.hit<-m.fa<-0
for(j in 1:n.trials){
if(truth[j,i]==1)sig=sig+1
if(truth[j,i]==0)noise=noise+1
if(truth[j,i]==1 && resp[j,i]==1)hit=hit+1
if(truth[j,i]==0 && resp[j,i]==1)fa=fa+1
}
s.hfar[i,1]=hit/sig
s.hfar[i,2]=fa/noise
dprime[i]=qnorm(s.hfar[i,1])-qnorm(s.hfar[i,2])
bias[i]=exp(-((s.hfar[i,1])^2-qnorm(1-s.hfar[i,2])^2)/2)
}
list(hitrate=s.hfar[,1],farate=s.hfar[,2],dprime=dprime,bias=bias)
}
# slightly vectorized REM code.
oldway1=function(u=u,c=c,g=g,w=w,n.study,n.targets,n.test){
n.distractors=n.test-n.targets
v=matrix(0,w,n.study)
# generate the study list
study=matrix(rgeom(w*n.study,g)+1,w,n.study)
# set up the test list
targets=study[,1:n.targets]
distractors=matrix(rgeom(w*n.distractors,g)+1,w,n.distractors)
probe=cbind(targets,distractors)
lambda=matrix(NA,n.study,n.test)
last=array(NA,c(w,n.study,n.test))
# this is the storage process in REM at study
for(j in 1:n.study){
for(k in 1:w){
flip=0
if(flip==0){
if(runif(1)<u){
flip=1
if(runif(1)<c){v[k,j]=study[k,j]} else {v[k,j]=rgeom(1,g)+1}
}}
}}
# this is the feature comparison process at test
for(i in 1:n.test){
for(j in 1:n.study){
for(k in 1:w){
if(v[k,j]==0)last[k,j,i]=1
if(v[k,j]!=0){
if(v[k,j]==probe[k,i]){last[k,j,i]=(c+(1-c)*g*(1-g)^(v[k,j]-1))/(g*(1-g)^(v[k,j]-1))
} else {last[k,j,i]=1-c}
}}
lambda[j,i]=prod(last[,j,i])
}}
# Bayesian decision rule
as.numeric(apply(lambda,2,mean)>1)
}
oldway2=function(u=u,c=c,g=g,w=w,n.study,n.targets,n.test){
n.distractors=n.test-n.targets
v=numeric(w*n.study)
study=numeric(w*n.study)
distractors=numeric(w*n.distractors)
probe=numeric(w*n.test)
fam=out=numeric(n.test)
lambda=numeric(n.study)
targets=numeric(n.targets)
# this is the study list
for(i in 1:w){
for(j in 1:n.study){
study[(i-1)*n.study+j]=rgeom(1,g)+1
}}
# these are the items from study that will be presented at test
for(i in 1:w){
for(j in 1:n.targets){
targets[(i-1)*n.targets+j]=study[(i-1)*n.targets+j]
}}
# these are the items that are presented at test, not from the study list
for(i in 1:w){
for(j in 1:n.distractors){
distractors[(i-1)*n.distractors+j]=rgeom(1,g)+1
}}
# the next two blocks constructs the test list. targets go first, then distractors
for(i in 1:w){
for(j in 1:n.targets){
probe[(i-1)*n.targets+j]=study[(i-1)*n.targets+j]
}}
for(i in 1:w){
for(j in 1:n.distractors){
probe[(i-1)*n.distractors+j+n.targets*w]=distractors[(i-1)*n.distractors+j]
}}
# this is the storage process in REM at study
for(i in 1:n.study){
for(j in 1:w){
r1=runif(1.0)
r2=runif(1.0)
if(r1>=u)v[(i-1)*w+j]=0.0
if(r1<u & r2<c)v[(i-1)*w+j]=study[(i-1)*w+j]
if(r1<u & r2>=c)v[(i-1)*w+j]=rgeom(1,g)+1
}}
# this is the feature comparison process at test
for(i in 1:n.test){
lambda=0
for(j in 1:n.study){
odds=1
for(k in 1:w){
if(v[(j-1)*w+k]==0)temp=1
if(v[(j-1)*w+k]!=0 & v[(j-1)*w+k]==probe[(i-1)*w+k])temp=(c+(1-c)*g*(1-g)^(v[(j-1)*w+k]-1))/(g*(1-g)^(v[(j-1)*w+k]-1))
if(v[(j-1)*w+k]!=0 & v[(j-1)*w+k]!=probe[(i-1)*w+k])temp=1-c
odds=odds*temp
}
lambda=lambda+odds
}
# calculation of familiarity
fam[i]=1/n.study*lambda
# Bayesian decision rule
if(fam[i]>=1)out[i]=1
if(fam[i]<1)out[i]=0
}
out
}
rem_c=function(u,c,g,w,n.study,n.targets,n.test){
n.distractors=n.test-n.targets
.C("rem",u=u,c=c,g=g,ge=g,w=w,n_study=n.study,n_targets=n.targets,n_test=n.test,n_distractors=n.distractors,out=numeric(n.test))
}
############################################################
dyn.load("rem.so")
g=.45 # environmental g parameter
u=.4  # probabilty of storing something
c=.7  # probability of storing features correctly
w=20  # length of vector storing features for the items
subs=1 # how many subjects?
# length of study list
n.study =rep(25,subs)
# length of test list
n.test =rep(20,subs)
# how many items in test list are targets?
n.targets=rep(11,subs)
# true state of stimuli (for d' calculation)
truth=unlist(lapply(1:subs,function(x,target,dist)c(rep(1,target[x]),rep(0,dist[x])),target=n.targets,dist=n.test-n.targets))
# simulate data from REM with C
rem_c(u=u,c=c,g=g,w=w,n.study,n.targets,n.test)
############################################################ perform a simulation to test effeciency of three methods
nmc=500 # how many times to simulate from REM, per method?
# storage objects
rates=array(NA,c(nmc,2,3))
timer=numeric(3)
########### using slightly vectorized code
ptm=proc.time()[3]
for(i in 1:nmc){
# simulate from model
out=oldway1(u=u,c=c,g=g,w=w,n.study,n.targets,n.test)
# evaluate performance
temp=dprime(out,truth=c(rep(1,n.targets),rep(0,n.test-n.targets)))
# store info
rates[i,,1]=c(temp$farate,temp$hitrate)
if(i%%10==0)print(i)
}
timer[1]=proc.time()[3]-ptm
col=rgb(0,0,1,.25)
par(mfrow=c(1,3))
plot(jitter(rates[,1,1]),jitter(rates[,2,1]),xlim=c(0,1),ylim=c(0,1),col=col,pch=16,xlab="False Alarm Rate",ylab="Hit Rate")
abline(0,1)
plot(jitter(rates[,1,2]),jitter(rates[,2,2]),xlim=c(0,1),ylim=c(0,1),col=col,pch=16,xlab="False Alarm Rate",ylab="Hit Rate")
abline(0,1)
plot(jitter(rates[,1,3]),jitter(rates[,2,3]),xlim=c(0,1),ylim=c(0,1),col=col,pch=16,xlab="False Alarm Rate",ylab="Hit Rate")
abline(0,1)
par(mfrow=c(1,1))
plot(jitter(rates[,1,1]),jitter(rates[,2,1]),xlim=c(0,1),ylim=c(0,1),col=col,pch=16,xlab="False Alarm Rate",ylab="Hit Rate")
abline(0,1)
sse.tmp <- function(x, data){
if (x["tau"] > 0){
# library(tidyverse)
startx <- c(1, 1)
n.data <- dim(data)[1]
n.items <- 2 # left and right
n.sim <- 10
dt <- .1
eta <- .000001
nu <- 0
sim.array <- array(NA, dim = c(n.sim, n.data, n.items))
for(i in 1:n.sim){
sim.array[i, , ] <- rlcca(n.items = 2, max.time = n.data, startx = startx,
drift = cbind(data$drift1, data$drift2),
K = x["K"], L = x["L"], nu = nu, eta = eta, thresh = 1000,
dt = dt, tau = x["tau"], t0 = .2)$state
}
mean.accum <- apply(sim.array, c(2, 3), mean)
mean.accum[, 1] <- normalit(mean.accum[, 1])
mean.accum[, 2] <- normalit(mean.accum[, 2])
response <- normalit(data$response)
sum((mean.accum[, 1] - response)^2) - sum((mean.accum[, 2] - response)^2)
} else{
-Inf
}
}
# dynamic models
thresh=100              # threshold
L=.4   						# lateral inhibition
K=.1							# leakage
nu=0                  # feed forward inhibition
xi=1                  # this is xi/sqrt(dt/tau)
tau=.15               # nondecision time
drift = drift          # drift rate
startx=c(1,1)         # starting point
I0=.1
delta_t=.1            # change in time constant
dt=.1             # step size
maxtime=5           # total amount of time before terminating
n.data <- dim(data)[1]
n.items <- 2 # left and right
n.sim <- 10
sim.array <- array(NA, dim = c(n.sim, n.data, n.items))
data=read_csv(here("evidenceAccumulationMaster.csv"))
sub.name <- "Chris Responses"
sub.data <- select(data, Trial, Coherence, sub.name)
library(here)
source(here("LCCA.R"))
# dynamic models
thresh=100              # threshold
L=.4   						# lateral inhibition
K=.1							# leakage
nu=0                  # feed forward inhibition
xi=1                  # this is xi/sqrt(dt/tau)
tau=.15               # nondecision time
drift = drift          # drift rate
startx=c(1,1)         # starting point
I0=.1
delta_t=.1            # change in time constant
dt=.1             # step size
maxtime=5           # total amount of time before terminating
n.data <- dim(data)[1]
n.items <- 2 # left and right
n.sim <- 10
sim.array <- array(NA, dim = c(n.sim, n.data, n.items))
data=read_csv(here("evidenceAccumulationMaster.csv"))
sub.name <- "Chris Responses"
sub.data <- select(data, Trial, Coherence, sub.name)
colnames(sub.data)[3] <- "response"
library(tidyverse)
library(here)
source(here("LCCA.R"))
# dynamic models
thresh=100              # threshold
L=.4   						# lateral inhibition
K=.1							# leakage
nu=0                  # feed forward inhibition
xi=1                  # this is xi/sqrt(dt/tau)
tau=.15               # nondecision time
drift = drift          # drift rate
startx=c(1,1)         # starting point
I0=.1
delta_t=.1            # change in time constant
dt=.1             # step size
maxtime=5           # total amount of time before terminating
n.data <- dim(data)[1]
n.items <- 2 # left and right
n.sim <- 10
sim.array <- array(NA, dim = c(n.sim, n.data, n.items))
data=read_csv(here("evidenceAccumulationMaster.csv"))
sub.name <- "Chris Responses"
sub.data <- select(data, Trial, Coherence, sub.name)
colnames(sub.data)[3] <- "response"
sub.data  <- sub.data %>%
mutate(drift1 = ifelse(Coherence>0, Coherence, 0),
drift2 = ifelse(Coherence<0, -Coherence, 0),
Iter = seq_along(Coherence))
sse.tmp <- function(x, data){
if (x["tau"] > 0){
# library(tidyverse)
startx <- c(1, 1)
n.data <- dim(data)[1]
n.items <- 2 # left and right
n.sim <- 10
dt <- .1
eta <- .000001
nu <- 0
sim.array <- array(NA, dim = c(n.sim, n.data, n.items))
for(i in 1:n.sim){
sim.array[i, , ] <- rlcca(n.items = 2, max.time = n.data, startx = startx,
drift = cbind(data$drift1, data$drift2),
K = x["K"], L = x["L"], nu = nu, eta = eta, thresh = 1000,
dt = dt, tau = x["tau"], t0 = .2)$state
}
mean.accum <- apply(sim.array, c(2, 3), mean)
mean.accum[, 1] <- normalit(mean.accum[, 1])
mean.accum[, 2] <- normalit(mean.accum[, 2])
response <- normalit(data$response)
sum((mean.accum[, 1] - response)^2) - sum((mean.accum[, 2] - response)^2)
} else{
-Inf
}
}
x <- NULL
x$K <- K
# x$eta <- eta
# x$dt <- dt
x$L <- L
x$tau <- tau
# sse.tmp(unlist(x), sub.data)
# optim(init, log.dens.like, data=data, control=list("fnscale"=-1,"maxit"=1000))
optim(unlist(x), sse.tmp, data=sub.data, control=list("fnscale"=-1,"maxit"=1000))
# L=.4   						# lateral inhibition
# K=.1							# leakage
K = 0.028231186
L = 0.571976671
tau = 0.007626444
nu=0                  # feed forward inhibition
xi=1                  # this is xi/sqrt(dt/tau)
startx=c(1,1)         # starting point
I0=.1
delta_t=.1            # change in time constant
dt=.1             # step size
maxtime=5           # total amount of time before terminating
n.data <- dim(data)[1]
n.items <- 2 # left and right
n.sim <- 10
sim.array <- array(NA, dim = c(n.sim, n.data, n.items))
data=read_csv(here("evidenceAccumulationMaster.csv"))
sub.name <- "Chris Responses"
sub.data <- select(data, Trial, Coherence, sub.name)
colnames(sub.data)[3] <- "response"
sub.data  <- sub.data %>%
mutate(drift1 = ifelse(Coherence>0, Coherence, 0),
drift2 = ifelse(Coherence<0, -Coherence, 0),
Iter = seq_along(Coherence))
for(i in 1:n.sim){
sim.array[i, , ] <- rlcca(n.items = 2, max.time = n.data, startx = startx,
drift = bind_cols(list("drift1"=sub.data$drift1, "drift2"=sub.data$drift2)),
K = K, L = L, nu = nu, eta = .01, thresh = 1000, dt = dt, tau = tau, t0 = .2)$state
}
mean.accum <- apply(sim.array, c(2, 3), mean)
colnames(mean.accum) <- c("mean_accum1", "mean_accum2")
sub.data <- cbind(sub.data, mean.accum)
sub.data <- sub.data %>% mutate(Response_norm = normalit(response),
mean_accum1_norm = normalit(mean_accum1),
mean_accum2_norm = normalit(mean_accum2))
ggplot(sub.data) +
geom_line(aes(Iter, mean_accum1_norm), col = "red") +
geom_line(aes(Iter, -mean_accum2_norm), col = "blue") +
geom_line(aes(Iter, 2*(Response_norm-.5)), col = "black") +
ylab("Accumulators")
K
L
mean.accum
mean.accum[,1]
mean.accum[,1]>mean.accum[,2]
length(mean.accum)
dim(mean.accum)
dim(mean.accum)[1]
t1 <- numeric(dim(mean.accum)[1])
t1
t2 <- mean.accum[,1]<mean.accum[,2]
t3 <- mean.accum[,1]>mean.accum[,2]
t2
t1[t2] <- mean.accum[t2,1]
t1[t3] <- mean.accum[t3,1]
x11(14,7)
plot(t1, type = "")
plot(t1, type = "l")
t1[t3] <- -mean.accum[t3,2]
plot(t1, type = "l")
t1
View(t2)
View(t3)
ggplot(sub.data) +
geom_line(aes(Iter, mean_accum1_norm), col = "red") +
geom_line(aes(Iter, -mean_accum2_norm), col = "blue") +
geom_line(aes(Iter, 2*(Response_norm-.5)), col = "black") +
ylab("Accumulators")
n.sim <- 1
for(i in 1:n.sim){
sim.array[i, , ] <- rlcca(n.items = 2, max.time = n.data, startx = startx,
drift = bind_cols(list("drift1"=sub.data$drift1, "drift2"=sub.data$drift2)),
K = K, L = L, nu = nu, eta = .01, thresh = 1000, dt = dt, tau = tau, t0 = .2)$state
}
mean.accum <- apply(sim.array, c(2, 3), mean)
sub.data$mean_accum1 <- mean.accum[,1]
sub.data$mean_accum2 <- mean.accum[,2]
sub.data <- sub.data %>% mutate(Response_norm = normalit(response),
mean_accum1_norm = normalit(mean_accum1),
mean_accum2_norm = normalit(mean_accum2))
ggplot(sub.data) +
geom_line(aes(Iter, mean_accum1_norm), col = "red") +
geom_line(aes(Iter, -mean_accum2_norm), col = "blue") +
geom_line(aes(Iter, 2*(Response_norm-.5)), col = "black") +
ylab("Accumulators")
ggplot(sub.data) +
geom_line(aes(Iter, mean_accum1_norm), col = "red") +
geom_line(aes(Iter, -mean_accum2_norm), col = "blue") +
geom_line(aes(Iter, 2*(Response_norm-.5)), col = "black") +
ylab("Accumulators")
range(data$`Chris Responses`)
range(data$`Matt Responses`)
